{
   "markdown" : "### [Careers](https://towardsai.net/p/category/careers), [Data Science](https://towardsai.net/p/category/careers), [Machine Learning](https://towardsai.net/p/category/machine-learning)\n\n# Why itâs Super Hard to be an ML Researcher or Developer?\n\n### A Realization that Literally Changed my Life\n\n![Source: Image by the author (made using Canva)](https://miro.medium.com/1*Ot1HMgLOe8GhwX1Knlx96g.png)\n\n---\n\n<iframe src=\"https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fanchor.fm%2Fnishu-talks%2Fembed%2Fepisodes%2FWhy-its-Super-Hard-to-be-an-ML-Researcher-or-Developer-emsrpv&display_name=Anchor+FM+Inc.&url=https%3A%2F%2Fanchor.fm%2Fnishu-talks%2Fepisodes%2FWhy-its-Super-Hard-to-be-an-ML-Researcher-or-Developer-emsrpv&image=https%3A%2F%2Fd3t3ozftmdmh3i.cloudfront.net%2Fproduction%2Fpodcast_uploaded_nologo400%2F10759800%2F10759800-1606157708107-a5012566fbed8.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=anchor\" title=\"Listen and Read\" height=\"102\" width=\"400\"></iframe>\n\nAccording to the [stats](https://data-mining.philippe-fournier-viger.com/too-many-machine-learning-papers/), currently, 33,000 ML Research papers are being published per year. That means, around 90+ papers per day! And almost every single paper introduces something new, without any exception.\n\n![Source: \"Deep Learning to Solve Challenging Problems\" (Google AI)](https://miro.medium.com/1*d_FD7HIUkOfGFppGsKScHQ.jpeg)\n\nAs an \"AI enthusiast\", I try to get a brief (or a rough idea) of as many papers as possible to keep myself updated in the industry. But to be honest, it feels a bit overwhelming.\n\nTo make things much worse, I recently heard a \"story\" that literally changed my life as an ML/DL developer.\n\n**Warning:**What Iâm about to share with you may disturb your career as an ML researcher/developer or make you deeply anxious. So, hear my reasoning until the end.\n\n> You may love it or hate it, but you canât ignore it\n\nAfter realizing what I mentioned above, I was already feeling like, \"Have I wasted a good amount of my life with ML research? Could I do something more useful with my life?\". And then â¦\n\n---\n\n## The Story that changed my life\n\nA few months ago, I came across this **true story** of a Harvard professor of political science, Gary King, who started working on the document clustering problem to give a Festschrift (_a collection of writings published in honor of a scholar_) to one of his colleagues, as the retirement gift.\n\nTo do so, he asked his grad students to utilize every clustering algorithm ever invented. For those who know about this, clustering is a very old problem in the field of machine learning and statistics. Hence, there were plenty of methods available to apply in the literature, and they found around 250 algorithms.\n\nTo compare the efficiency of all the algorithms, they coded an R package and what did they found? Was there any absolute \"best\" algorithm?\n\nNope. Obviously not.\n\nAs expected, each method worked differently. They could not decide which algorithm was best, and at last, they let their users pick the results that they themselves found useful.\n\nSo what did I learn?\n\n---\n\n## Listen Carefully\n\nHere, I described the situation using clustering, but the same argument can be given to any problem - may that be reinforcement learning, deep learning, supervised learning, unsupervised learning, or anything else.\n\nRight now, I am pretty sure that there are more than a hundred variants of SGD (Stochastic Gradient Descent) alone, which is an integral part of deep learning.\n\nAnd that is scary to me (even with some experience in this field).\n\nIt made me ask this question: _Should I spend my time inventing the 251st clustering algorithm?_\n\nWe all know that **_glory comes to the trailblazers_**. Every next version of something gets lesser and lesser credit (like a [submodular set function](https://en.wikipedia.org/wiki/Submodular_set_function)); the law of diminishing returns applies.\n\nAfter Ian Goodfellow invented [GANs](https://en.wikipedia.org/wiki/Generative_adversarial_network) (Generative Adversarial Networks), there were more than a hundred types of GAN variants out there. Everyone was attracted to them and contributed, but unfortunately, only a few got some recognition. Ian Goodfellow will always be at the center of the GAN universe.\n\nSo let me ask you this again, _Is it really worth it to invent the 251st clustering algorithm or a 101st SGD variant?_\n\n---\n\n## The ML Tragedy\n\n![Source: Image by the author (made using Canva)](https://miro.medium.com/1*ptTQf1mSQFngCpFqKUZVdA.png)\n\nI told you, there is no happily ever after.\n\nThere is a set of theorems in search and optimization called, \"[**No Free Lunch Theorems**](https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization)\" (seriously, no kidding) which accurately depicts our situation. Let me quote this from Wikipedia:\n\n> In computational complexity and optimization the **no free lunch theorem** is a result that states that for certain types of mathematical problems, the computational cost of finding a solution, averaged over all problems in the class, is the same for any solution method. **No solution therefore offers a \"short cut\".** This is under the assumption that the search space is a probability density function. It does not apply to the case where the search space has underlying structure that can be exploited more efficiently than random search or even has closed-form solutions that can be determined without search at all. **For such probabilistic assumptions, the outputs of all procedures solving a particular type of problem are statistically identical**. A colourful way of describing such a circumstance, introduced by David Wolpert and William G. Macready in connection with the problems of search and optimization, is to say that **there is no free lunch**. Wolpert had previously derived **no free lunch theorem**s for machine learning (statistical inference). Before Wolpertâs article was published, Cullen Schaffer independently proved a restricted version of one of Wolpertâs theorems and used it to critique the current state of machine learning research on the problem of induction [[1](https://wikimili.com/en/No_free_lunch_in_search_and_optimization)].\n\nIn short, the theorem says, there isnât gonna be any \"best\" approach (or algorithm) that can fit all problem space.\n\nAfter averaging over a large input distribution, every algorithm performs the same, more or less. Hence, no best clustering algorithm, no best RL (reinforcement learning) method, no best regressor, etcâ¦ Itâs all hokum.\n\n---\n\n## Whatâs Next?\n\nSince now I have put you in my shoes, you must be wondering why you read all this?\n\nBecause there **is**something that you can do! And that is, to **change your approach.**\n\nAs an ML developer, the best advice that I can give to you is: Instead of concentrating on ML algorithms, focus on the problem - **Problem Formulation must be your main priority**.\n\n> Give me six hours to chop down a tree and I will spend the first four sharpening the axe - Abraham Lincoln\n\nDonât approach the problem from the opposite direction. I learned that the hard way, but once I did, it changed everything.\n\n> [**How I Won a National Level ML Competition with my Unique \"Informal Approach\"**](https://towardsdatascience.com/how-i-won-a-national-level-ml-competition-with-my-unique-informal-approach-e86fd95532fd)\n\nKeep that in mind and Iâm sure youâll **not** get lost (into this algorithm jungle). I hope youâll spend your time more wisely from this point forward.\n\n---\n\nIf you enjoy reading these stories, then Iâm sure you would love to be a [Medium paying member](https://nishu-jain.medium.com/membership). Itâs only $5 per month, and youâll get unlimited access to thousands and thousands of stories and writers. You can support me by [**signing up using this link**](https://nishu-jain.medium.com/membership),_and Iâll earn a small commission that will help me grow and publish more stories like this._\n\n_Thanks for reading and have a nice day!_\n\n---\n\n**Other awesome articles that you might enjoy -**\n\n> [**Machine Learning is Becoming a Joke**](https://medium.com/datadriveninvestor/machine-learning-is-becoming-a-joke-automl-downsides-c7634ce0572c)\n\n> [**10 Game-changing AI Breakthroughs Worth Knowing About**](https://pub.towardsai.net/10-game-changing-ai-breakthroughs-worth-knowing-about-b2076afc4930)\n\n> [**Now AI is Knocking On The Doors of Luxurious Hotels**](https://medium.com/towards-artificial-intelligence/now-ai-is-knocking-on-the-doors-of-luxurious-hotels-2066b1102590)"
}
