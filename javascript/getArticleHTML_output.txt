Article HTML: {
  id: '67fa62fc1971',
  html: `<html lang=en><head><title>Why it’s Super Hard to be an ML Researcher or Developer?</title><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width initial-scale=1.0"><meta name="description" content="A Realization that Literally Changed my Life"><meta name="keywords" content="data-science, machine-learning, artificial-intelligence, deep-learning, careers"><meta name="author" content="Nishu Jain"><meta property="og:type" content="website"><meta property="og:title" content="Why it’s Super Hard to be an ML Researcher or Developer?"><meta property="og:description" content="A Realization that Literally Changed my Life"><meta property="og:image" content="https://miro.medium.com/1*Ot1HMgLOe8GhwX1Knlx96g.png"><meta property="twitter:card" content="summary"><meta property="twitter:title" content="Why it’s Super Hard to be an ML Researcher or Developer?"><meta property="twitter:description" content="A Realization that Literally Changed my Life"><meta property="twitter:image" content="https://miro.medium.com/1*Ot1HMgLOe8GhwX1Knlx96g.png"><link rel="stylesheet" href="dark.css"></head><body><div class="blog"><h3 id="0908"><a href="https://towardsai.net/p/category/careers" target="_blank">Careers</a>, <a href="https://towardsai.net/p/category/careers" target="_blank">Data Science</a>, <a href="https://towardsai.net/p/category/machine-learning" target="_blank">Machine Learning</a></h3><h1 id="92ef">Why it's Super Hard to be an ML Researcher or Developer?</h1><h3 id="f84f">A Realization that Literally Changed my Life</h3><img alt="Source: Image by the author (made using Canva)" src="https://miro.medium.com/1*Ot1HMgLOe8GhwX1Knlx96g.png"></img><hr><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fanchor.fm%2Fnishu-talks%2Fembed%2Fepisodes%2FWhy-its-Super-Hard-to-be-an-ML-Researcher-or-Developer-emsrpv&display_name=Anchor+FM+Inc.&url=https%3A%2F%2Fanchor.fm%2Fnishu-talks%2Fepisodes%2FWhy-its-Super-Hard-to-be-an-ML-Researcher-or-Developer-emsrpv&image=https%3A%2F%2Fd3t3ozftmdmh3i.cloudfront.net%2Fproduction%2Fpodcast_uploaded_nologo400%2F10759800%2F10759800-1606157708107-a5012566fbed8.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=anchor" title="Listen and Read" height="102" width="400"></iframe><p>According to the <a href="https://data-mining.philippe-fournier-viger.com/too-many-machine-learning-papers/" target="_blank">stats</a>, currently, 33,000 ML Research papers are being published per year. That means, around 90+ papers per day! And almost every single paper introduces something new, without any exception.</p><img alt="Source: "Deep Learning to Solve Challenging Problems" (Google AI)" src="https://miro.medium.com/1*d_FD7HIUkOfGFppGsKScHQ.jpeg"></img><p>As an "AI enthusiast", I try to get a brief (or a rough idea) of as many papers as possible to keep myself updated in the industry. But to be honest, it feels a bit overwhelming.</p><p>To make things much worse, I recently heard a "story" that literally changed my life as an ML/DL developer.</p><p><b>Warning:</b> What I'm about to share with you may disturb your career as an ML researcher/developer or make you deeply anxious. So, hear my reasoning until the end.</p><blockquote class="big"><p>You may love it or hate it, but you can't ignore it</p></blockquote><p>After realizing what I mentioned above, I was already feeling like, "Have I wasted a good amount of my life with ML research? Could I do something more useful with my life?". And then ...</p><hr><h2 id="df3e">The Story that changed my life</h2><p>A few months ago, I came across this <b>true story</b> of a Harvard professor of political science, Gary King, who started working on the document clustering problem to give a Festschrift (<i>a collection of writings published in honor of a scholar</i>) to one of his colleagues, as the retirement gift.</p><p>To do so, he asked his grad students to utilize every clustering algorithm ever invented. For those who know about this, clustering is a very old problem in the field of machine learning and statistics. Hence, there were plenty of methods available to apply in the literature, and they found around 250 algorithms.</p><p>To compare the efficiency of all the algorithms, they coded an R package and what did they found? Was there any absolute "best" algorithm?</p><p>Nope. Obviously not.</p><p>As expected, each method worked differently. They could not decide which algorithm was best, and at last, they let their users pick the results that they themselves found useful.</p><p>So what did I learn?</p><hr><h2 id="ab11">Listen Carefully</h2><p>Here, I described the situation using clustering, but the same argument can be given to any problem - may that be reinforcement learning, deep learning, supervised learning, unsupervised learning, or anything else.</p><p>Right now, I am pretty sure that there are more than a hundred variants of SGD (Stochastic Gradient Descent) alone, which is an integral part of deep learning.</p><p>And that is scary to me (even with some experience in this field).</p><p>It made me ask this question: <i>Should I spend my time inventing the 251st clustering algorithm?</i></p><p>We all know that <i><b>glory comes to the trailblazers</b></i>. Every next version of something gets lesser and lesser credit (like a <a href="https://en.wikipedia.org/wiki/Submodular_set_function" target="_blank">submodular set function</a>); the law of diminishing returns applies.</p><p>After Ian Goodfellow invented <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network" target="_blank">GANs</a> (Generative Adversarial Networks), there were more than a hundred types of GAN variants out there. Everyone was attracted to them and contributed, but unfortunately, only a few got some recognition. Ian Goodfellow will always be at the center of the GAN universe.</p><p>So let me ask you this again, <i>Is it really worth it to invent the 251st clustering algorithm or a 101st SGD variant?</i></p><hr><h2 id="70c8">The ML Tragedy</h2><img alt="Source: Image by the author (made using Canva)" src="https://miro.medium.com/1*ptTQf1mSQFngCpFqKUZVdA.png"></img><p>I told you, there is no happily ever after.</p><p>There is a set of theorems in search and optimization called, "<b><a href="https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization" target="_blank">No Free Lunch Theorems</a></b>" (seriously, no kidding) which accurately depicts our situation. Let me quote this from Wikipedia:</p><blockquote class="small"><p>In computational complexity and optimization the <b>no free lunch theorem</b> is a result that states that for certain types of mathematical problems, the computational cost of finding a solution, averaged over all problems in the class, is the same for any solution method. <b>No solution therefore offers a "short cut".</b> This is under the assumption that the search space is a probability density function. It does not apply to the case where the search space has underlying structure that can be exploited more efficiently than random search or even has closed-form solutions that can be determined without search at all. <b>For such probabilistic assumptions, the outputs of all procedures solving a particular type of problem are statistically identical</b>. A colourful way of describing such a circumstance, introduced by David Wolpert and William G. Macready in connection with the problems of search and optimization, is to say that <b>there is no free lunch</b>. Wolpert had previously derived no free lunch theorems for machine learning (statistical inference). Before Wolpert's article was published, Cullen Schaffer independently proved a restricted version of one of Wolpert's theorems and used it to critique the current state of machine learning research on the problem of induction [<a href="https://wikimili.com/en/No_free_lunch_in_search_and_optimization" target="_blank">1</a>].</p></blockquote><p>In short, the theorem says, there isn't gonna be any "best" approach (or algorithm) that can fit all problem space.</p><p>After averaging over a large input distribution, every algorithm performs the same, more or less. Hence, no best clustering algorithm, no best RL (reinforcement learning) method, no best regressor, etc... It's all hokum.</p><hr><h2 id="16cb">What's Next?</h2><p>Since now I have put you in my shoes, you must be wondering why you read all this?</p><p>Because there <b>is</b> something that you can do! And that is, to <b>change your approach.</b></p><p>As an ML developer, the best advice that I can give to you is: Instead of concentrating on ML algorithms, focus on the problem - <b>Problem Formulation must be your main priority</b>.</p><blockquote class="big"><p>Give me six hours to chop down a tree and I will spend the first four sharpening the axe - Abraham Lincoln</p></blockquote><p>Don't approach the problem from the opposite direction. I learned that the hard way, but once I did, it changed everything.</p><div class="embed-link"><a href="https://towardsdatascience.com/how-i-won-a-national-level-ml-competition-with-my-unique-informal-approach-e86fd95532fd" target="_blank">How I Won a National Level ML Competition with my Unique "Informal Approach"</a></div><p>Keep that in mind and I'm sure you'll <b>not</b> get lost (into this algorithm jungle). I hope you'll spend your time more wisely from this point forward.</p><hr><p><i>If you enjoy reading these stories, then I'm sure you would love to be a <b><a href="https://nishu-jain.medium.com/membership" target="_blank">Medium paying member</a>.</b> It's only $5 per month, and you'll get unlimited access to thousands and thousands of stories and writers. You can support me by <b><a href="https://nishu-jain.medium.com/membership" target="_blank">signing up using this link</a></b></i>, <i>and I'll earn a small commission that will help me grow and publish more stories like this.</i></p><p><i>Than`... 883 more characters
}
